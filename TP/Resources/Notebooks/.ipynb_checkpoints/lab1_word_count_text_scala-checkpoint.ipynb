{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px;font-weight: bold;color:blue\">ESIPE, \"INFO 3-Opt° Logiciel\" </p>\n",
    "<p style=\"font-size:15px;font-weight: bold;color:blue\">Calcul distribué (Map-Reduce Spark) (Autumn 2019) -- Instructors: F. Sarradin, B. Men</p>\n",
    "\n",
    "**Sources**: These labs synthetize and *builds on* labs from several origins: \n",
    "- The series of moocs from Berkeley and Databricks,(Creative Commons licences), namely\n",
    "   - [Introduction to Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/info)\n",
    "   - [Big data Analysis with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/info)\n",
    "   - [Distributed Machine Learning with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS120x+2T2016/info)\n",
    "   - [Introduction to Big Data with Apache Spark](https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info)\n",
    "   - [Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info)\n",
    "- [Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning](https://github.com/jadianes/spark-py-notebooks) (Apache License, Version 2.0)\n",
    "\n",
    "We have kept the labs text in english. This will enable us to reuse them in international sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:35px;font-weight: bold;\"> Lab 1 :  Word Count with Spark</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will enable us to develop a simple word count application.  The volume of unstructured text in existence is growing dramatically, and Spark is an excellent tool for analyzing this type of data.  In this lab, we will write code that calculates the most common words in the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) retrieved from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page).  This could also be scaled to find the most common words on the Internet.\n",
    "\n",
    "** During this lab we will cover: **\n",
    "\n",
    "- **Part 1:** Word Count with RDDs  -- You will play with basic RDDs, then build a pair RDD, use it for counting words. Finally you will learn how to clean and prepare the RDD and build the application to count words in a file. \n",
    "\n",
    "- **Part 2:** is devoted to a small tutorial on Spark Dataframes (introduced in Spark 1.3)\n",
    "\n",
    "- **Part 3:** Word Count with dataframes -- You will follow the same steps as in Part 1 to develop a Word Counting application and apply it to a file. \n",
    "\n",
    "\n",
    "**Exercises** will include an explanation of what is expected, followed by code cells where one cell will have one or more `???` sections.  The cell that needs to be modified will have `// TODO: Replace ??? with appropriate code` on its first line.  Once the `???` sections are updated and the code is run, the test cell can then be run to verify the correctness of your solution.  The last code cell before the next markdown section will contain the tests.\n",
    "\n",
    "Note that, for reference, you can look up the details of the relevant methods in [Spark's Scala API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prerequisites : Spark Context configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "\n",
    "val spark = SparkSession.builder\n",
    ".appName(\"lab1_word_count_text\")\n",
    ".master(\"local[*]\")\n",
    ".getOrCreate()\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Word Count with RDDs  \n",
    "\n",
    "##  1. Creating a base RDD and pair RDDs \n",
    "\n",
    "We will first explore creating a base RDD with `parallelize` and using pair RDDs to count words.\n",
    "\n",
    "\n",
    "** (1a) Create a base RDD **\n",
    "\n",
    "We'll start by generating a base RDD by using a List and the `sc.parallelize` method.  Then we'll print out the type of the base RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordsList = List(\"cat\", \"elephant\", \"rat\", \"rat\", \"cat\")\n",
    "val wordsRDD = sc.parallelize(wordsList, 4)\n",
    "// Print out the type of wordsRDD\n",
    "println(wordsRDD.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (1b) Pluralize and test **\n",
    "\n",
    "Let's use a `map()` transformation to add the letter 's' to each string in the base RDD we just created. We'll define a function that returns the word with an 's' at the end of the word.  Please replace `???` with your solution.  If you have trouble, the next cell has the solution.  After you have defined `makePlural` you can run the third cell which contains a test.  If you implementation is correct it will print `1 test passed`.\n",
    "\n",
    "This is the general form that exercises will take, except that no example solution will be provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//TODO: Replace ??? with appropriate code\n",
    "def makePlural(word : String): String = {\n",
    "    /*Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  No attempt is made to follow proper\n",
    "        pluralization rules.\n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    */\n",
    "    return ???\n",
    "}\n",
    "    \n",
    "\n",
    "print(makePlural(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//One way of completing the function\n",
    "val makePlural = (word : String) => \n",
    "    word + \"s\"\n",
    "\n",
    "print(makePlural(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertResult[A](expected : A, error : String)(answer : A) = {\n",
    "    if (expected equals answer) \"1 test passed\"\n",
    "    else error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "# Load in the testing code and check to see if your answer is correct\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "*/\n",
    "\n",
    "//TEST Pluralize and test (1b)\n",
    "assertResult(\"rats\", \"incorrect result: makePlural does not add an s\") {\n",
    "      makePlural(\"rat\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (1c) Apply `makePlural` to the base RDD **\n",
    "\n",
    "Now pass each item in the base RDD into a [map()](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD@map[U](f:T=%3EU)(implicitevidence$3:scala.reflect.ClassTag[U]):org.apache.spark.rdd.RDD[U]) transformation that applies the `makePlural()` function to each element. And then call the [collect()](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD@collect():Array[T]) action to see the transformed RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val pluralRDD = wordsRDD.map(???)\n",
    "pluralRDD.collect().map(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//TEST Apply makePlural to the base RDD(1c)\n",
    "assertResult(List(\"cats\", \"elephants\", \"rats\", \"rats\", \"cats\"), \"incorrect values for pluralRDD\") {\n",
    "      pluralRDD.collect.toList\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (1d) Pass a `lambda` function to `map` **\n",
    "\n",
    "Let's create the same RDD using a `lambda` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val pluralLambdaRDD = wordsRDD.map(???)\n",
    "pluralLambdaRDD.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//TEST Pass a lambda function to map (1d)\n",
    "assertResult(List(\"cats\", \"elephants\", \"rats\", \"rats\", \"cats\"), \"incorrect values for pluralLambdaRDD (1d)\") {\n",
    "    pluralLambdaRDD.collect.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (1e) Length of each word **\n",
    "\n",
    "Now use `map()` and a `lambda` function to return the number of characters in each word.  We'll `collect` this result directly into a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val pluralLengths = pluralRDD\n",
    "  ???\n",
    "  .collect\n",
    "pluralLengths.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Length of each word (1e)\n",
    "assertResult(List(4, 9, 4, 4, 4), \"incorrect values for pluralLengths\") {\n",
    "  pluralLengths.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (1f) Pair RDDs **\n",
    "\n",
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.\n",
    "\n",
    "We can create the pair RDD using the `map()` transformation with a `lambda()` function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val wordPairs = wordsRDD.???\n",
    "wordPairs.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertResult(List((\"cat\", 1), (\"elephant\", 1), (\"rat\", 1), (\"rat\", 1), (\"cat\", 1)), \"incorrect values for wordPairs\") {\n",
    "  wordPairs.collect.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Counting with pair RDDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient than others.\n",
    "\n",
    "A naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, we want an approach that will work for any size dataset including terabyte- or petabyte-sized datasets. In addition, performing all of the work in the driver program is slower than performing it in parallel in the workers. For these reasons, we will use data parallel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2a) `groupByKey()` approach **\n",
    "\n",
    "\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions@groupByKey():org.apache.spark.rdd.RDD[(K,Iterable[V])]) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n",
    "  + The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    " \n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterable)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "// Note that groupByKey requires no parameters\n",
    "val wordsGrouped = wordPairs.???\n",
    "val keyValues = wordsGrouped.collect\n",
    "keyValues.foreach(x => println(x._1,x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST groupByKey() approach (2a)\n",
    "assertResult(List((\"cat\", List(1, 1)), (\"elephant\", List(1)), (\"rat\", List(1, 1))),\n",
    "\"incorrect value for wordsGrouped\") {\n",
    "  wordsGrouped.mapValues(_.toList).collect.sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2b) Use `groupByKey()` to obtain the counts **\n",
    "\n",
    "<p>Using the `groupByKey()` transformation creates an RDD containing 3 elements, each of which is a pair of a word and an Iterable.<br/>\n",
    "Now sum the Iterable using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val wordCountsGrouped = wordsGrouped.???\n",
    "wordCountsGrouped.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Use groupByKey() to obtain the counts (2b)\n",
    "assertResult(List((\"cat\", 2), (\"elephant\", 1), (\"rat\", 2)),\n",
    "\"incorrect value for wordCountsGrouped\") {\n",
    "  wordCountsGrouped.collect.sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2c) Counting using `reduceByKey` **\n",
    "\n",
    "A better approach is to start from the pair RDD and then use the [reduceByKey()](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions@reduceByKey(func:(V,V)=%3EV):org.apache.spark.rdd.RDD[(K,V)]) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "// Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "val wordCounts = wordPairs.???\n",
    "wordCounts.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Counting using reduceByKey (2c)\n",
    "assertResult(List((\"cat\", 2), (\"elephant\", 1), (\"rat\", 2)),\n",
    "  \"incorrect value for wordCounts\") {\n",
    "  wordCounts.collect.sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (2d) All together **\n",
    "\n",
    "The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect` in one statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val wordCountsCollected = wordsRDD\n",
    "                          .???\n",
    "  .collect()\n",
    "wordCountsCollected.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST All together (2d)\n",
    "assertResult(List((\"cat\", 2), (\"elephant\", 1), (\"rat\", 2)),\n",
    "\"incorrect value for wordCountsCollected\") {\n",
    "  wordCountsCollected.sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Finding unique words and a mean value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (3a) Unique words **\n",
    "\n",
    "Calculate the number of unique words in `wordsRDD`.  You can use other RDDs that you have already created to make this easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val uniqueWords = ???\n",
    "println(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Unique words (3a)\n",
    "assertResult(3, \"incorrect count of uniqueWords\") {\n",
    "  uniqueWords.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (3b) Mean using `reduce` **\n",
    "\n",
    "<p>Find the mean number of words per unique word in `wordCounts`.<br/>\n",
    "\n",
    "Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words.  First `map()` the pair RDD `wordCounts`, which consists of (key, value) pairs, to an RDD of values.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "import scala.math.BigDecimal.RoundingMode\n",
    "val totalCount = (wordCounts\n",
    "  .map(???)\n",
    "  .reduce(???))\n",
    "val average = totalCount / ???.toFloat\n",
    "println(totalCount)\n",
    "println(BigDecimal(average).setScale(2, RoundingMode.HALF_UP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Mean using reduce (3b)\n",
    "assertResult( BigDecimal(1.67), \"incorrect value of average\") {\n",
    "  BigDecimal(average).setScale(2, RoundingMode.HALF_UP)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Apply word count to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will finish developing our word count application.  We'll have to build the `wordCount` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4a) `wordCount` function **\n",
    "\n",
    "First, define a function for word counting.  You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in an RDD that is a list of words like `wordsRDD` and return a pair RDD that has all of the words and their associated counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "import org.apache.spark.rdd.RDD\n",
    "def wordCount(wordListRDD: RDD[String]) : RDD[(String,Int)] = {\n",
    "  /*Creates a pair RDD with word counts from an RDD of words.\n",
    "  Args:\n",
    "  wordListRDD (RDD of str): An RDD consisting of words.\n",
    "  Returns:\n",
    "  RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "  */\n",
    "  ???\n",
    "}\n",
    "wordCount(wordsRDD).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST wordCount function (4a)\n",
    "assertResult(List((\"cat\", 2), (\"elephant\", 1), (\"rat\", 2)),\n",
    "\"incorrect definition for wordCount function\") {\n",
    "  wordCount(wordsRDD).collect.sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4b) Capitalization and punctuation **\n",
    "\n",
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "\n",
    "  +  Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "  +  All punctuation should be removed.\n",
    "  +  Any leading or trailing spaces on a line should be removed.\n",
    " \n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use `replaceAll` to remove any text that is not a letter, number, or space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "val removePunctuation = (text:String) => {\n",
    "  /*\n",
    "Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "Note:\n",
    "    Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "    eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "    punctuation is removed.\n",
    "Args:\n",
    "    text (str): A string.\n",
    "Returns:\n",
    "    str: The cleaned up string.\n",
    "   */\n",
    "  val text_no_punctuation = text.replaceAll(\"[^a-zA-Z0-9 ]+\",\"\")\n",
    "  ???\n",
    "}\n",
    "println(removePunctuation(\"Hi, you!\"))\n",
    "println(removePunctuation(\" No under_score!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertResult(\"the elephants 4 cats\",\n",
    "  \"incorrect definition for removePunctuation function\") {\n",
    "  removePunctuation(\" The Elephant's 4 cats. \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4c) Load a text file **\n",
    "\n",
    "For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, so that we only print 15 lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Just run this code\n",
    "val fileName = \"../data/shakespeare.txt\"\n",
    "val shakespeareRDD = (sc\n",
    "  .textFile(fileName, 8)\n",
    "  .map(removePunctuation))\n",
    "shakespeareRDD\n",
    "  .zipWithIndex()  // to (line, lineNum)\n",
    "  .map(x => s\"${x._2}:${x._1}\")  // to 'lineNum: line'\n",
    ".take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4d) Words from lines **\n",
    "\n",
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the RDD:\n",
    "\n",
    "  +  The first issue is that  that we need to split each line by its spaces.\n",
    "  +  The second issue is we need to filter out empty lines.\n",
    " \n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply the string [split()](https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#split-java.lang.String-) function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be. Use an other transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val shakespeareWordsRDD = shakespeareRDD.???\n",
    "val shakespeareWordCount = shakespeareWordsRDD.count\n",
    "shakespeareWordsRDD.top(5).foreach(println)\n",
    "println(shakespeareWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Words from lines (4d)\n",
    "// This test allows for leading spaces to be removed either before or after punctuation is removed.\n",
    "assertResult(true, \"incorrect value for shakespeareWordCount\") {\n",
    "  shakespeareWordCount == 927631 || shakespeareWordCount == 928908\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4e) Remove empty elements **\n",
    "\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val shakeWordsRDD = shakespeareWordsRDD.???\n",
    "val shakeWordCount = shakeWordsRDD.count\n",
    "println(shakeWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Remove empty elements (4e)\n",
    "assertResult(882996 , \"incorrect value for shakeWordCount\") {\n",
    "  shakeWordCount.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4f) Count the vocabulary size **\n",
    "\n",
    "<p> How many different words (vocabulary size) are there in Shakespeare's vocabulary? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// let us get the complete Shakespare' vocabulary\n",
    "val shakespare_vocab_size = ???\n",
    "println(s\"Shakespeare vocabulary contains $shakespare_vocab_size words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Count the vocabulary size (4f)\n",
    "assertResult(28147, \"incorrect value for shakespare_vocab_size\") {\n",
    " shakespare_vocab_size.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (4g) Get the top words **\n",
    "\n",
    "We now have an RDD that is only words.  Next, let's apply the `wordCount()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair.\n",
    "\n",
    "<p>You'll notice that many of the words are common English words. These are called stopwords. In a later lab, we will see how to eliminate them from the results.<br/>\n",
    "Use the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO Replace ??? with appropriate code\n",
    "val top15WordsAndCounts = ???\n",
    "top15WordsAndCounts.foreach(x => println(s\"${x._1}: ${x._2}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Count the words (4f)\n",
    "assertResult(\n",
    "  List((\"the\", 27361), (\"and\", 26028), (\"i\", 20681), (\"to\", 19150), (\"of\", 17463),\n",
    "(\"a\", 14593), (\"you\", 13615), (\"my\", 12481), (\"in\", 10956), (\"that\", 10890),\n",
    "(\"is\", 9134), (\"not\", 8497), (\"with\", 7771), (\"me\", 7769), (\"it\", 7678)),\n",
    "\"incorrect value for top15WordsAndCounts\") {\n",
    "  top15WordsAndCounts.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Short tutorial to DataFrames \n",
    "\n",
    "and chaining together transformations and actions\n",
    "\n",
    "## 1. Working with your first DataFrames\n",
    "\n",
    "In Spark, we first create a base [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package@DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]). We can then apply one or more transformations to that base DataFrame. *A DataFrame is immutable, so once it is created, it cannot be changed.* As a result, each transformation creates a new DataFrame. Finally, we can apply one or more actions to the DataFrames.\n",
    "\n",
    "> Note that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n",
    "\n",
    "We will perform several exercises to obtain a better understanding of DataFrames:\n",
    "* Create a collection of 10,000 people \n",
    "* Create a Spark DataFrame from that collection\n",
    "* Subtract one from each value using `map`\n",
    "* Perform action `collect` to view results\n",
    "* Perform action `count` to view counts\n",
    "* Apply transformation `filter` and view results with `collect`\n",
    "* Learn about lambda functions\n",
    "* Explore how lazy evaluation works and the debugging challenges that it introduces\n",
    "\n",
    "A DataFrame consists of a series of `Row` objects; each `Row` object has a set of named columns. You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table.\n",
    "\n",
    "More formally, a DataFrame must have a _schema_, which means it must consist of columns, each of which has a _name_ and a _type_. Some data sources have schemas built into them. Examples include RDBMS databases, Parquet files, and NoSQL databases like Cassandra. Other data sources don't have computer-readable schemas, but you can often apply a schema programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a collection of 10,000 people\n",
    "\n",
    "Normally in this section we would use a third-party Scala library to create a collection of fake person records. Since the import doesn't work well in the Scala version of the notebook, we will load a .csv file containing the records previously and ramdomly generated from the python version of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load a collection of randomly generated people records.\n",
    "In the next section, we'll turn that collection into a DataFrame.\n",
    "We'll use the `spark.implicits._`, because that will give us helpers to create a DataFrame from a List for example.\n",
    "There are other ways to define schemas, though; see\n",
    "the Spark Programming Guide's discussion of [schema inference](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection) for more information. (In here, we can directly create a dataframe from reading the csv file, but for the purpose of this section we will create a dataframe from a RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "val dataRDD = sc.textFile(\"../data/10000persons.csv\").map(_.split(\";\")).map(x => (x(0),x(1),x(2),x(3),x(4).toInt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed data and using a collection to create a DataFrame\n",
    "\n",
    "In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine.  Each partition holds a unique subset of the entries in the list.  Spark calls datasets that it stores \"Resilient Distributed Datasets\" (RDDs). Even DataFrames are ultimately represented as RDDs, with additional meta-data.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png\" style=\"width: 900px; float: right; margin: 5px\"/>\n",
    "\n",
    "One of the defining features of Spark, compared to other data analytics frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.  This allows Spark applications to run much more quickly, because they are not slowed down by needing to read data from disk.\n",
    "The figure to the right illustrates how Spark breaks a list of data entries into partitions that are each stored in memory on a worker.\n",
    "\n",
    "\n",
    "To create the DataFrame, we'll use `sqlContext.createDataFrame()`, and we'll pass our array of data in as an argument to that function. Spark will create a new set of input data based on data that is passed in.  A DataFrame requires a _schema_, which is a list of columns, where each column has a name and a type. Our list of data has elements with types (mostly strings, but one integer). We'll supply the rest of the schema and the column names as the argument to `toDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataDF = dataRDD.toDF(\"last_name\", \"first_name\", \"ssn\", \"occupation\", \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the DataFrame's schema and some of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many partitions will the DataFrame be split into?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subtract one from each value using _select_\n",
    "\n",
    "So far, we've created a distributed DataFrame that is splitted into many partitions, where each partition is stored on a single machine in our cluster.  Let's look at what happens when we do a basic operation on the dataset.  Many useful data analysis operations can be specified as \"do something to each item in the dataset\".  These data-parallel operations are convenient because each item in the dataset can be processed individually: the operation on one entry doesn't effect the operations on any of the other entries.  Therefore, Spark can parallelize the operation.\n",
    "\n",
    "One of the most common DataFrame operations is `select()`, and it works more or less like a SQL `SELECT` statement: You can select specific columns from the DataFrame, and you can even use `select()` to create _new_ columns with values that are derived from existing column values. We can use `select()` to create a new column that decrements the value of the existing `age` column.\n",
    "\n",
    "`select()` is a _transformation_. It returns a new DataFrame that captures both the previous DataFrame and the operation to add to the query (`select`, in this case). But it does *not* actually execute anything on the cluster. When transforming DataFrames, we are building up a _query plan_. That query plan will be optimized, implemented (in terms of RDDs), and executed by Spark _only_ when we call an action (see Lazy Evaluation process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age'\n",
    "// Because select is a transformation and Spark uses lazy evaluation, no jobs, stages, or tasks will be launched when we run this code.\n",
    "val subDF = dataDF.select(dataDF(\"last_name\"), dataDF(\"first_name\"), dataDF(\"ssn\"), dataDF(\"occupation\"), (dataDF(\"age\") - 1).alias(\"age\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.  Use _collect_ or _show_ to view results\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "To see a list of elements decremented by one, we need to create a new list on the driver from the the data distributed in the executor nodes.  To do this we can call the `collect()` method on our DataFrame.  `collect()` is often used after transformations to ensure that we are only returning a *small* amount of data to the driver.  This is done because the data returned to the driver must fit into the driver's available memory.  If not, the driver will crash.\n",
    "\n",
    "The `collect()` method is the first action operation that we have encountered.  Action operations cause Spark to perform the (lazy) transformation operations that are required to compute the values returned by the action.  In our example, this means that tasks will now be launched to perform the `toDF`, `select`, and `collect` operations.\n",
    "\n",
    "In the diagram, the dataset is broken into four partitions, so four `collect()` tasks are launched. Each task collects the entries in its partition and sends the result to the driver, which creates a list of the values, as shown in the figure below.\n",
    "\n",
    "Now let's run `collect()` on `subDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Let's collect the data\n",
    "val results = subDF.collect()\n",
    "results.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF.show() // look at parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use _count_ to get total\n",
    "\n",
    "One of the most basic jobs that we can run is the `count()` job which will count the number of elements in a DataFrame, using the `count()` action. Since `select()` creates a new DataFrame with the same number of elements as the starting DataFrame, we expect that applying `count()` to each DataFrame will return the same result.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png\" style=\"height:750px;float:right\"/>\n",
    "\n",
    "Note that because `count()` is an action operation, if we had not already performed an action with `collect()`, then Spark would now perform the transformation operations when we executed `count()`.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure on the right shows what would happen if we ran `count()` on a small example dataset with just four partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println (dataDF.count())\n",
    "println (subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply transformation _filter_ and view results with _collect_\n",
    "\n",
    "Next, we'll create a new DataFrame that only contains the people whose ages are less than 10. To do this, we'll use the `filter()` transformation. (You can also use `where()`, an alias for `filter()`, if you prefer something more SQL-like). The `filter()` method is a transformation operation that creates a new DataFrame from the input DataFrame, keeping only values that match the filter expression.\n",
    "\n",
    "The figure shows how this might work on the small four-partition dataset.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3f.png\" style=\"height:700px;float:right\"/>\n",
    "\n",
    "\n",
    "- To view the filtered list of elements less than 10, we need to create a new list on the driver from the distributed data on the executor nodes.  We use the `collect()` method to return a list that contains all of the elements in this filtered DataFrame to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filteredDF = subDF.filter(subDF(\"age\") < 10)\n",
    "filteredDF.show(truncate=false)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional DataFrame actions\n",
    "\n",
    "Let's investigate some additional actions:\n",
    "\n",
    "* [first()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@first():T)\n",
    "* [take()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@take(n:Int):Array[T])\n",
    "\n",
    "One useful thing to do when we have a new dataset is to look at the first few entries to obtain a rough idea of what information is available.  In Spark, we can do that using actions like `first()`, `take()`, and `show()`. Note that for the `first()` and `take()` actions, the elements that are returned depend on how the DataFrame is *partitioned*.\n",
    "\n",
    "Instead of using the `collect()` action, we can use the `take(n)` action to return the first _n_ elements of the DataFrame. The `first()` action returns the first element of a DataFrame, and is equivalent to `take(1)[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"first: ${filteredDF.first()}\")\n",
    "println(s\"Four of them: ${filteredDF.take(4).mkString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional DataFrame transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  _orderBy_**\n",
    "\n",
    "[`orderBy()`](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@orderBy(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]) allows you to sort a DataFrame by one or more columns, producing a new DataFrame.\n",
    "\n",
    "For example, let's get the first five oldest people in the original (unfiltered) DataFrame. We can use the `orderBy()` transformation. `orderBy` takes one or more columns, either as _names_ (strings) or as `Column` objects. To get a `Column` object, we use one of two notations on the DataFrame:\n",
    "\n",
    "* Subscript notation: `filteredDF(\"age\")`\n",
    "\n",
    "Both of those syntaxes return a `Column`, which has additional methods like `desc()` (for sorting in descending order) or `asc()` (for sorting in ascending order, which is the default).\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "```\n",
    "dataDF.orderBy(dataDF(\"age\"))  // sort by age in ascending order; returns a new DataFrame\n",
    "dataDF.orderBy(dataDF(\"last_name\").desc) // sort by last name in descending order\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get the five oldest people in the list. To do that, sort by age in descending order.\n",
    "dataDF.orderBy(dataDF(\"age\").desc).take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  _distinct_ and _dropDuplicates_**\n",
    "\n",
    "[`distinct()`](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@distinct():org.apache.spark.sql.Dataset[T]) filters out duplicate rows, and it considers all columns. Since our data is completely randomly generated (by `fake-factory`), it's extremely unlikely that there are any duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(dataDF.count())\n",
    "println(dataDF.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** _drop_**\n",
    "\n",
    "[`drop()`](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@drop(col:org.apache.spark.sql.Column):org.apache.spark.sql.DataFrame) is like the opposite of `select()`: Instead of selecting specific columns from a DataFrame, it drops a specifed column from a DataFrame.\n",
    "\n",
    "Here's a simple use case: Suppose you're reading from a 1,000-column CSV file, and you have to get rid of five of the columns. Instead of selecting 995 of the columns, it's easier just to drop the five you don't want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_groupBy_**\n",
    "\n",
    "[`groupBy()`](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset) is one of the most powerful transformations. It allows you to perform aggregations on a DataFrame.\n",
    "\n",
    "Unlike other DataFrame transformations, `groupBy()` does _not_ return a DataFrame. Instead, it returns a special [RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset) object that contains various aggregation functions.\n",
    "\n",
    "The most commonly used aggregation function is [count()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset@count():org.apache.spark.sql.DataFrame),\n",
    "but there are others (like [sum()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset@sum(colNames:String*):org.apache.spark.sql.DataFrame), [max()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset@max(colNames:String*):org.apache.spark.sql.DataFrame), and [avg()](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset@avg(colNames:String*):org.apache.spark.sql.DataFrame).\n",
    "\n",
    "These aggregation functions typically create a new column and return a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.groupBy(\"occupation\").count().show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : WordCount with Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Create a DataFrame_**\n",
    "\n",
    "We'll start by generating a base DataFrame by using a List of Strings and the `toDF` method.  Then we'll print out the type and schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val wordsDF = List(\"cat\",\"elephant\",\"rat\",\"rat\",\"cat\").toDF(\"word\")\n",
    "wordsDF.show()\n",
    "println(wordsDF.getClass)\n",
    "wordsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counting with Spark SQL and DataFrames \n",
    "\n",
    "Now, let's count the number of times a particular word appears in the 'word' column. There are multiple ways to perform the counting, but some are much less efficient than others.\n",
    "\n",
    "A naive approach would be to call `collect` on all of the elements and count them in the driver program. While this approach could work for small datasets, we want an approach that will work for any size dataset including terabyte- or petabyte-sized datasets. In addition, performing all of the work in the driver program is slower than performing it in parallel in the workers. For these reasons, we will use data parallel operations.\n",
    "\n",
    "**_Using `groupBy` and `count`_**\n",
    "\n",
    "Using DataFrames, we can perform aggregations by grouping the data using the [`groupBy` function](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@groupBy(col1:String,cols:String*):org.apache.spark.sql.RelationalGroupedDataset) on the DataFrame.  Using `groupBy` returns a [`RelationalGroupedDataset` object](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset) and we can use the functions available for `GroupedData` to aggregate the groups.  For example, we can call `avg` or `count` on a `GroupedData` object to obtain the average of the values in the groups or the number of occurrences in the groups, respectively.\n",
    "\n",
    "To find the counts of words, group by the words and then use the [`count` function](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@count():Long) to find the number of times that words occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val wordCountsDF = wordsDF.???\n",
    "wordCountsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST groupBy and count (2a)\n",
    "import org.apache.spark.sql.Row\n",
    "assertResult(List((\"cat\",2), (\"elephant\",1), (\"rat\", 2)), \"incorrect counts for wordCountsDF\") {\n",
    "  wordCountsDF.collect().map({case Row(s : String, c : Long) => (s,c.toInt)}).sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding unique words and a mean value \n",
    "\n",
    "** _Unique words_ **\n",
    "\n",
    "Calculate the number of unique words in `wordsDF`.  You can use other DataFrames that you have already created to make this easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val uniqueWordsCount = ???\n",
    "println(uniqueWordsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:27: error: not found: value assertResult\n",
       "       assertResult(3, \"incorrect count of unique words\") {\n",
       "       ^\n",
       "<console>:28: error: not found: value uniqueWordsCount\n",
       "         uniqueWordsCount.toInt\n",
       "         ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TEST Unique words\n",
    "assertResult(3, \"incorrect count of unique words\") {\n",
    "  uniqueWordsCount.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apply word count to a file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will finish developing our word count application.  We'll have to build the `wordCount` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data.\n",
    "\n",
    "**  The `wordCount` function **\n",
    "\n",
    "First, define a function for word counting.  You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in a DataFrame that is a list of words like `wordsDF` and return a DataFrame that has all of the words and their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "def wordCountDF(wordListDF : DataFrame) = {\n",
    "  /*\n",
    "Creates a DataFrame with word counts.\n",
    "Args:\n",
    "    wordListDF (DataFrame of str): A DataFrame consisting of one string column c\n",
    "Returns:\n",
    "    DataFrame of (str, int): A DataFrame containing 'word' and 'count' columns.\n",
    "   */\n",
    " ???\n",
    "}\n",
    "wordCountDF(wordsDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST wordCount function (4a)\n",
    "assertResult(List((\"cat\", 2), (\"elephant\", 1), (\"rat\", 2)), \"incorrect definition for wordCountDF function\") {\n",
    "  wordCountDF(wordsDF).collect().map({case Row(s : String, c : Long) => (s,c.toInt)}).sortBy(_._1).toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  _Capitalization and punctuation_ **\n",
    "\n",
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "  + All punctuation should be removed.\n",
    "  + Any leading or trailing spaces on a line should be removed.\n",
    "\n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  \n",
    "\n",
    "To complete this task, you should use Pyspark implemented functions such as :\n",
    "\n",
    "[regexp_replace](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$@regexp_replace(e:org.apache.spark.sql.Column,pattern:String,replacement:String):org.apache.spark.sql.Column) module to remove any text that is not a letter, number, or space. If you are unfamiliar with regular expressions, refer to the `removePunctuation` function defined previously in this notebook. You may also want to review [this tutorial](https://developers.google.com/edu/python/regular-expressions) from Google (in Python).  Also, [this website](https://regex101.com/#python) is  a great resource for debugging your regular expression. \n",
    "\n",
    "You should also use the `trim` and `lower` functions found in [pyspark.sql.functions] in order to lower text and remove leading and trailing spaces. (https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).\n",
    "\n",
    "> Note that you shouldn't use any RDD operations or need to create custom user defined functions (udfs) to accomplish this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// TODO: Replace ??? with appropriate code\n",
    "def removePunctuationDF(coll : Column) = {\n",
    "  /*\n",
    "Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "Note:\n",
    "    Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "    eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "    punctuation is removed.\n",
    "Args:\n",
    "    column (Column): A Column containing a sentence.\n",
    "Returns:\n",
    "    Column: A Column named 'sentence' with clean-up operations applied.\n",
    "   */\n",
    "  lower(trim(regexp_replace(coll, \"[^\\\\w\\\\s\\\\d]*\",\"\")))\n",
    "}\n",
    "val sentenceDF = List((\"Hi, you!\"),(\" No under_score!\"), (\" *      Remove punctuation then spaces  * \"))\n",
    "  .toDF(\"sentence\")\n",
    "sentenceDF.show(truncate = false)\n",
    "sentenceDF\n",
    "  .select(removePunctuationDF(sentenceDF(\"sentence\")))\n",
    "  .show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** _Load a text file_ **\n",
    "\n",
    "For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into a DataFrame, we use the `sqlContext.read.text()` method. We also apply the recently defined `removePunctuation()` function using a `select()` transformation to strip out the punctuation and change all text to lower case.  Since the file is large we use `show(15)`, so that we only print 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shakespeareDF = spark.sqlContext.read.text(fileName).select(removePunctuationDF(col(\"value\")))\n",
    "shakespeareDF.show(15, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** _Words from lines_ **\n",
    "\n",
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the DataFrame:\n",
    "  + The first issue is that  that we need to split each line by its spaces.\n",
    "  + The second issue is we need to filter out empty lines or words.\n",
    "\n",
    "Apply a transformation that will split each 'sentence' in the DataFrame by its spaces, and then transform from a DataFrame that contains lists of words into a DataFrame with each word in its own row.  To accomplish these two tasks you can use the `split` and `explode` functions found in [spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).\n",
    "\n",
    "Once you have a DataFrame with one word per row you can apply the [DataFrame operation `where`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@where(conditionExpr:String):org.apache.spark.sql.Dataset[T]) to remove the rows that contain ''.\n",
    "\n",
    "> Note that `shakeWordsDF` should be a DataFrame with one column named `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val shakeWordsDF :DataFrame = (shakespeareDF.???)\n",
    "shakeWordsDF.show()\n",
    "val shakeWordsDFCount = shakeWordsDF.count()\n",
    "println(shakeWordsDFCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Remove empty elements (4d)\n",
    "assertResult(882996, \"incorrect value for shakeWordCount\") {\n",
    "  shakeWordsDFCount.toInt\n",
    "}\n",
    "assertResult(List(\"word\"),\"shakeWordsDF should only contain the Column 'word'\") {\n",
    "  shakeWordsDF.columns.toList\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  _Count the words_ **\n",
    "\n",
    "We now have a DataFrame that is only words.  Next, let's apply the `wordCount()` function to produce a list of word counts. We can view the first 20 words by using the `show()` action; however, we'd like to see the words in descending order of count, so we'll need to apply the [`orderBy` DataFrame method](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset@orderBy(sortExprs:org.apache.spark.sql.Column*):org.apache.spark.sql.Dataset[T]) to first sort the DataFrame that is returned from `wordCount()`.\n",
    "\n",
    "You'll notice that many of the words are common English words. These are called stopwords. In a later lab, we will see how to eliminate them from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace ??? with appropriate code\n",
    "val topWordsAndCountsDF = ???\n",
    "topWordsAndCountsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TEST Count the words (4e)\n",
    "assertResult(\n",
    "  List((\"the\", 27361), (\"and\", 26028), (\"i\", 20681), (\"to\", 19150), (\"of\", 17463),\n",
    "    (\"a\", 14593), (\"you\", 13615), (\"my\", 12481), (\"in\", 10956), (\"that\", 10890),\n",
    "    (\"is\", 9134), (\"not\", 8497), (\"with\", 7771), (\"me\", 7769), (\"it\", 7678)),\n",
    "  \"incorrect value for top15WordsAndCountsDF\") {\n",
    "  topWordsAndCountsDF.take(15).map({\n",
    "    case Row(s:String,c:Long) => (s,c.toInt)\n",
    "  }).toList\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {
   "height": "371px",
   "width": "433px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
